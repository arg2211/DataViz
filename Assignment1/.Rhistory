acqmyrac = sort(unique(acqmyrac)),
polviews = sort(unique(polviews)))
pred.dat <- with(r2, expand.grid(
bdiscr = sort(unique(bdiscr)),
age = mean(age),
age.sq = mean(age.sq),
educ = mean(educ),
realinc = mean(realinc),
acqmyrac = sort(unique(acqmyrac)),
polviews = sort(unique(polviews))))
pred.dat <- with(r2, expand.grid(
bdiscr = sort(unique(bdiscr)),
age = mean(age),
age.sq = mean(age.sq),
educ = mean(educ),
realinc = mean(realinc),
acqmyrac = sort(unique(acqmyrac)),
polviews = sort(unique(polviews))))
predProb(logit2, predData = pred.dat, ci = F)
library(visreg)
plot(logit2)
library(visreg)
visreg(logit2, xvar="bdiscr", by="bmotiv", overlay=T, partial=F, legend=F,
xlab="Ineq. Due to Discrimination", ylab="Ineq. Due to Lack of Motivation",
scale="response", line = list(col = c("cyan3", "purple3", "red", "blue", "gray55")))
visreg(logit2, xvar="bdiscr", by="bmotiv", overlay=T, partial=F, legend=F,
xlab="Ineq. Due to Discrimination", ylab="Ineq. Due to Lack of Motivation",
scale="response", type = "conditional",
line = list(col = c("cyan3", "purple3", "red", "blue", "gray55")))
visreg(logit2, xvar="bdiscr", by="bmotiv", overlay=T, partial=F, legend=F,
xlab="Ineq. Due to Discrimination", ylab="Ineq. Due to Lack of Motivation",
scale="response", type = "conditional", alpha = .90,
line = list(col = c("cyan3", "purple3", "red", "blue", "gray55")))
visreg(logit2, xvar="bdiscr", by="bmotiv", overlay=T, partial=F, legend=F,
xlab="Ineq. Due to Discrimination", ylab="Ineq. Due to Lack of Motivation",
scale="response", type = "conditional", alpha = .90,
line = list(col = c("cyan3", "purple3")))
visreg(logit2, xvar="bdiscr", by="bmotiv", overlay=T, partial=F, legend=F,
xlab="Ineq. Due to Discrimination", ylab="Ineq. Due to Lack of Motivation",
scale="response", type = "conditional", alpha = .90,
line = list(col = c("cyan3", "purple3", "gray55")))
legend("bottomleft", lwd = 2, c("1", "2", "3", "4", "5"), col = c("cyan3", "purple3", "red", "blue", "gray55"), cex = 0.8)
library(plm)
library(QMSS)
d=read.csv(file.choose()) ## choose "panel-for-R.csv" and more information on variables are here: http://sda.berkeley.edu/sdaweb/analysis/?dataset=gss06panelw3 ##
vars <- c("idnum","panelwave","affrmact","race", "intrace1")
pd.sub <- d[, vars]
pd.sub$black = ifelse(pd.sub$race==2, 1, 0)
pd.sub$intblack = ifelse(pd.sub$intrace1==2, 1, 0)
pd.sub$r.affact = 5-pd.sub$affrmact
pd.sub$year= ifelse(pd.sub$panelwave==3, 1, 0)
lm1 <- lm(r.affact ~ black + intblack + as.factor(panelwave),  data = pd.sub)
summary(lm1)
clusterSE(fit = lm1, cluster.var = "idnum", data=pd.sub)
plm1 <- plm(r.affact ~ black + intblack + year,  index = c("idnum", "panelwave"),  model = "fd", data = pd.sub)
summary(plm1)
clusterSE(fit = plm1, cluster.var = "idnum", data=pd.sub)
pd.sub$fourvsall= ifelse(pd.sub$r.affact==4, 1, 0)
pd.sub$fourthreevsall= ifelse(pd.sub$r.affact>=3, 1, 0)
pd.sub$fourthreetwovsone= ifelse(pd.sub$r.affact>=2, 1, 0)
plm2 <- plm(fourvsall ~ black + intblack + year,  index = c("idnum", "panelwave"),  model = "fd", data = pd.sub)
plm3 <- plm(fourthreevsall ~ black + intblack + year,  index = c("idnum", "panelwave"),  model = "fd", data = pd.sub)
plm4 <- plm(fourthreetwovsone ~ black + intblack + year,  index = c("idnum", "panelwave"),  model = "fd", data = pd.sub)
summary(plm4)
library(stargazer)
stargazer(plm1, plm2, plm3, plm4, type = "text")
pd.sub$d.intblack = firstD(intblack, idnum, pd.sub )
table(pd.sub$d.intblack)
pd.sub$bw=ifelse(pd.sub$d.intblack==-1,1,0)
pd.sub$wb=ifelse(pd.sub$d.intblack==1,1,0)
pd.sub$d.r.affact=firstD(r.affact, idnum, pd.sub )
summary(lm(intblack ~ black, pd.sub))
pd.sub$d.black = firstD(black, idnum, pd.sub )
table(pd.sub$d.black)
summary(lm(d.r.affact ~ bw, pd.sub, subset=black==0))
vars2 = c("colsci", "colscinm", "advfront", "scibnfts", "natsci", "educ", "relig", "fund", "idnum", "panelwave", "id", "year")
v = pan[,vars2]
table(v$panelwave, v$advfront)
prop.table(table(v$panelwave, v$advfront))
table(v$panelwave, v$natsci)
prop.table(table(v$panelwave, v$natsci))
library(plyr)
library(devtools)
library(QMSS)
v$scinec = 5-v$advfront
lm2 <- lm(natsci ~ scinec + as.factor(panelwave), v)
summary(lm2)
sci.pool2 <- plm(natsci ~ scinec + as.factor(panelwave), index = c("idnum", "panelwave"), model = "pooling", data = v)
summary(sci.pool2)
clusterSE(fit = lm2, cluster.var = "idnum", data = v)
pan=read.csv(file.choose()) ## choose panel-for-R.csv
vars2 = c("colsci", "colscinm", "advfront", "scibnfts", "natsci", "educ", "relig", "fund", "idnum", "panelwave", "id", "year")
v = pan[,vars2]
table(v$panelwave, v$advfront)
prop.table(table(v$panelwave, v$advfront))
library(plyr)
library(devtools)
library(QMSS)
v$scinec = 5-v$advfront
lm2 <- lm(natsci ~ scinec + as.factor(panelwave), v)
summary(lm2)
clusterSE(fit = lm2, cluster.var = "idnum", data = v)
v <- ddply(v, "idnum", mutate, d.scinec = firstD(scinec), d.natsci = firstD(natsci))
table(v$d.scinec)
prop.table(table(v$d.scinec))
table(v$d.natsci)
prop.table(table(v$d.natsci))
plm2 <- plm(natsci ~ scinec + as.factor(panelwave), index = c("idnum", "panelwave"), model = "fd", data = v)
summary(plm2)
clusterSE(fit = plm2, cluster.var = "idnum", data=v)
v$bsci = 4-v$scibnfts
v <- ddply(v, "idnum", mutate, d.bsci = firstD(bsci))
table(v$d.bsci)
prop.table(table(v$d.bsci))
plm3 <- plm(natsci ~ scinec + as.factor(panelwave) + as.factor(bsci), index = c("idnum", "panelwave"), model = "fd", data = v)
summary(plm3)
library(stargazer)
stargazer(lm2, plm2, plm3, title="Regression Results", align=TRUE, dep.var.labels=c("Federal Spending on Scientific Research"), covariate.labels=c("Pro Sci","2008","2010","Ben=Harm","Ben>Harm"), no.space=TRUE, column.labels=c("OLS", "First Diff A", "First Diff B"), dep.var.caption="", model.numbers=FALSE, type = "text", omit = "Constant")
install.packages("streamR")
library(streamR)
requestURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"
consumerKey <- "gYIxV6TdEf9glxYWlGSGdBAiF"
consumerSecret <- "QIRynpLw0P6elikgsRyQeM7SzencY0Q1oGDwfb4zvzWpGBUe1K"
my_oauth <- OAuthFactory$new(consumerKey = consumerKey, consumerSecret = consumerSecret,
requestURL = requestURL, accessURL = accessURL, authURL = authURL)
my_oauth$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))
library(RCurl)
requestURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"
consumerKey <- "gYIxV6TdEf9glxYWlGSGdBAiF"
consumerSecret <- "QIRynpLw0P6elikgsRyQeM7SzencY0Q1oGDwfb4zvzWpGBUe1K"
my_oauth <- OAuthFactory$new(consumerKey = consumerKey, consumerSecret = consumerSecret,
requestURL = requestURL, accessURL = accessURL, authURL = authURL)
load("my_oauth")
library(ROAuth)
install.packages("ROAuth")
library(ROAuth)
library(RCurl)
requestURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"
consumerKey <- "gYIxV6TdEf9glxYWlGSGdBAiF"
consumerSecret <- "QIRynpLw0P6elikgsRyQeM7SzencY0Q1oGDwfb4zvzWpGBUe1K"
my_oauth <- OAuthFactory$new(consumerKey = consumerKey, consumerSecret = consumerSecret,
requestURL = requestURL, accessURL = accessURL, authURL = authURL)
my_oauth$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))
save(my_oauth, file = "my_oauth.Rdata")
load("my_oauth.Rdata")
library(streamR)
filterStream("tweets.json", track = c("women", "men", timeout = 10, oauth = my_oauth))
filterStream("tweets.json", track = c("women", "men"), timeout = 10, oauth = my_oauth)
tweets.df <- parseTweets("tweets.json", simplify = TRUE)
c( length(grep("women", tweets.df$text, ignore.case = TRUE)),
length(grep("men", tweets.df$text, ignore.case = TRUE)) )
View(tweets.df)
table(tweets.df)
View(tweets.df)
list(tweets.df)
filterStream("tweetsUS.json", locations = c(-125, 25, -66, 50), timeout = 10,
oauth = my_oauth)
tweets.df <- parseTweets("tweetsUS.json", verbose = FALSE)
library(ggplot2)
library(grid)
map.data <- map_data("state")
install.packages("maps")
library(maps)
map.data <- map_data("state")
points <- data.frame(x = as.numeric(tweets.df$lon), y = as.numeric(tweets.df$lat))
points <- points[points$y > 25, ]
ggplot(map.data) + geom_map(aes(map_id = region), map = map.data, fill = "white",
color = "grey20", size = 0.25) + expand_limits(x = map.data$long, y = map.data$lat) +
theme(axis.line = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(),
axis.title = element_blank(), panel.background = element_blank(), panel.border = element_blank(),
panel.grid.major = element_blank(), plot.background = element_blank(),
plot.margin = unit(0 * c(-1.5, -1.5, -1.5, -1.5), "lines")) + geom_point(data = points,
aes(x = x, y = y), size = 1, alpha = 1/5, color = "darkblue")
table(gss$race)
prop.table(table(gss$race))
find.package("devtools")
library(devtools)
find_rtools()
find.package("KernSmooth")
library(KernSmooth)
getwd()
setwd("~/GitHub/NYTtextanalysis/data/random2015dates")
options(stringsAsFactors = FALSE)
library(tm)
nyt <- read.csv('nyt*.csv', header = TRUE, stringsAsFactors = FALSE)
nyt <- read.csv('nyt01-02-15.csv', header = TRUE, stringsAsFactors = FALSE)
nyt01 <- read.csv('nyt01-02-15.csv', header = TRUE, stringsAsFactors = FALSE)
nyt02 <- read.csv('nyt02-02-15.csv', header = TRUE, stringsAsFactors = FALSE)
nyt03 <- read.csv('nyt03-20-15.csv', header = TRUE, stringsAsFactors = FALSE)
nyt04 <- read.csv('nyt04-11-15.csv', header = TRUE, stringsAsFactors = FALSE)
nyt05 <- read.csv('nyt05-12-15.csv', header = TRUE, stringsAsFactors = FALSE)
nyt06 <- read.csv('nyt06-06-15.csv', header = TRUE, stringsAsFactors = FALSE)
nyt07 <- read.csv('nyt07-26-15.csv', header = TRUE, stringsAsFactors = FALSE)
nyt08 <- read.csv('nyt08-30-15.csv', header = TRUE, stringsAsFactors = FALSE)
nyt09 <- read.csv('nyt09-18-15.csv', header = TRUE, stringsAsFactors = FALSE)
nyt10 <- read.csv('nyt10-17-15.csv', header = TRUE, stringsAsFactors = FALSE)
nyt11 <- read.csv('nyt11-10-15.csv', header = TRUE, stringsAsFactors = FALSE)
nyt12 <- read.csv('nyt12-10-15.csv', header = TRUE, stringsAsFactors = FALSE)
View(nyt01)
View(nyt02)
nytcorpus <- VCorpus(DataframeSource(nyt01, nyt02))
nytcorpus <- Corpus(DirSource(./*.csv))
nytcorpus <- Corpus(DirSource(.))
nytcorpus <- Corpus(DirSource(nyt*.csv))
nytcorpus <- Corpus(DirSource())
nyt.merged <- cbind(nyt01, nyt02, nyt03, nyt04, nyt05, nyt06, nyt07, nyt08, nyt09, nyt10, nyt11, nyt12)
nyt.merged <- rbind(nyt01, nyt02, nyt03, nyt04, nyt05, nyt06, nyt07, nyt08, nyt09, nyt10, nyt11, nyt12)
nyt.merged <- rbind(nyt01, nyt02, nyt03, nyt04, nyt05, nyt06, nyt07, nyt08, nyt09, nyt10, nyt11, nyt12)
?merge()
View(nyt08)
row.names(nyt01)
column.names(nyt01)
colnames(nyt01)
colnames(nyt01, nyt02)
colnames(nyt02)
View(nyt01)
by.x = 'PUBLICATION')
nyt.merged <- merge(nyt01, nyt02, nyt03, nyt04, nyt05, nyt06, nyt07, nyt08, nyt09, nyt10, nyt11, nyt12, by.x = 'PUBLICATION')
all.x = TRUE)
nyt.merged <- merge(nyt01, nyt02, nyt03, nyt04, nyt05, nyt06, nyt07, nyt08, nyt09, nyt10, nyt11, nyt12, all.x = TRUE)
nyt.merged <- merge(nyt01, nyt02, nyt03, nyt04, nyt05, nyt06, nyt07, nyt08, nyt09, nyt10, nyt11, nyt12, by.x = "PUBLICATION", all.x = TRUE)
nyt.merged <- merge(nyt01, nyt02, nyt03, nyt04, nyt05, nyt06, nyt07, nyt08, nyt09, nyt10, nyt11, nyt12, by.x = C("PUBLICATION"), all.x = TRUE)
nyt.merged <- merge(nyt01, nyt02, nyt03, nyt04, nyt05, nyt06, nyt07, nyt08, nyt09, nyt10, nyt11, nyt12, by.x = C("TEXT"), all.x = TRUE)
colnames(nyt08)
View(nyt08)
df01 <- subset(nyt01, select = c(SEARCH_ROW))
View(df01)
df01 <- subset(nyt01, select = c(SEARCH_ROW, PUBLICATION, SECTION, DATE, TITLE, BYLINE, COUNTRY, STATE, CITY, PERSON, SUBJECT, LENGTH, TEXT))
View(df01)
df02 <- subset(nyt02, select = c(SEARCH_ROW, PUBLICATION, SECTION, DATE, TITLE, BYLINE, COUNTRY, STATE, CITY, PERSON, SUBJECT, LENGTH, TEXT))
df03 <- subset(nyt03, select = c(SEARCH_ROW, PUBLICATION, SECTION, DATE, TITLE, BYLINE, COUNTRY, STATE, CITY, PERSON, SUBJECT, LENGTH, TEXT))
df04 <- subset(nyt04, select = c(SEARCH_ROW, PUBLICATION, SECTION, DATE, TITLE, BYLINE, COUNTRY, STATE, CITY, PERSON, SUBJECT, LENGTH, TEXT))
df05 <- subset(nyt05, select = c(SEARCH_ROW, PUBLICATION, SECTION, DATE, TITLE, BYLINE, COUNTRY, STATE, CITY, PERSON, SUBJECT, LENGTH, TEXT))
df06 <- subset(nyt06, select = c(SEARCH_ROW, PUBLICATION, SECTION, DATE, TITLE, BYLINE, COUNTRY, STATE, CITY, PERSON, SUBJECT, LENGTH, TEXT))
df07 <- subset(nyt07, select = c(SEARCH_ROW, PUBLICATION, SECTION, DATE, TITLE, BYLINE, COUNTRY, STATE, CITY, PERSON, SUBJECT, LENGTH, TEXT))
df08 <- subset(nyt08, select = c(SEARCH_ROW, PUBLICATION, SECTION, DATE, TITLE, BYLINE, COUNTRY, STATE, CITY, PERSON, SUBJECT, LENGTH, TEXT))
df09 <- subset(nyt09, select = c(SEARCH_ROW, PUBLICATION, SECTION, DATE, TITLE, BYLINE, COUNTRY, STATE, CITY, PERSON, SUBJECT, LENGTH, TEXT))
df10 <- subset(nyt10, select = c(SEARCH_ROW, PUBLICATION, SECTION, DATE, TITLE, BYLINE, COUNTRY, STATE, CITY, PERSON, SUBJECT, LENGTH, TEXT))
df11 <- subset(nyt11, select = c(SEARCH_ROW, PUBLICATION, SECTION, DATE, TITLE, BYLINE, COUNTRY, STATE, CITY, PERSON, SUBJECT, LENGTH, TEXT))
df12 <- subset(nyt12, select = c(SEARCH_ROW, PUBLICATION, SECTION, DATE, TITLE, BYLINE, COUNTRY, STATE, CITY, PERSON, SUBJECT, LENGTH, TEXT))
nyt.merged <- rbind(nyt01, nyt02, nyt03, nyt04, nyt05, nyt06, nyt07, nyt08, nyt09, nyt10, nyt11, nyt12)
nyt.merged <- rbind(df01, df02, df03, df04, df05, df06, df07, df08, df09, df10, df11, df12)
View(nyt.merged)
nytcorpus <- VCorpus(DataframeSource(nyt.merged))
class(nytcorpus) #just making sure it's a corpus
inspect(nytcorpus) #gives info about all "documents" in corpus
inspect(nytcorpus[2]) #gives info about only the 2nd "document" in corpus
summary(nytcorpus)
onlytext <- paste(nyt.merged$TEXT, collapse = " ", stringsAsFactors = FALSE)
class(onlytext)
summary(onlytext)
onlytextvs <- VectorSource(onlytext)
onlytextcorpus <- Corpus(onlytextvs)
class(onlytextcorpus)
toSpace <- content_transformer(function(x, pattern) {return (gsub(pattern, " ", x))}) #creates function "toSpace" using gsub that replaces characters with a space
otc <- tm_map(onlytextcorpus, content_transformer(tolower)) #need to use content_transformer with tolower because of bug in newer version of tm package
otc <- tm_map(otc, removePunctuation)
otc <- tm_map(otc, toSpace, "-")
otc <- tm_map(otc, toSpace, ":")
otc <- tm_map(otc, stripWhitespace)
otc <- tm_map(otc, removeWords, stopwords("english"))
inspect(otc)
class(otc)
dtm <- DocumentTermMatrix(otc)
dtm2 <- as.matrix(dtm)
freq <- colSums(dtm2)
str(freq)
freq <- sort(freq, decreasing = TRUE)
head(freq)
library(wordcloud)
words <- names(freq)
wordcloud(words[1:100], freq[1:100])
wordcloud(words[1:200], freq[1:200])
wordcloud(words[1:200], freq[1:100])
dtm
dtm2 # gives number of terms in documents
summary(dtm2) # gives number of terms in documents
dtm # gives number of terms in documents
head(table(freq), 20)
head(table(freq), 30) #shows you a table of frequencies
wordcloud(names(freq), freq, max.words=100)
wordcloud(names(freq), freq, min.freq =100)
wordcloud(names(freq), freq, min.freq =1000)
library(RColorBrewer)
wordcloud(names(freq), freq, min.freq=1000, colors=brewer.pal(8, "Dark2")) #creates word cloud of words, by frequency (larger text = more), with only words that occur 1000+ times
art <- c(141, 145, 255, 167, 204, 171, 354, 341, 216, 169, 189, 230)
mean(art)
barplot(art, width = 1,
names.arg = c("Jan-2", "Feb-2", "Mar-20", "Apr-11", "May-12", "Jun-6", "Jul-26", "Aug-30", "Sept-18", "Oct-17", "Nov-10", "Dec-10", "Avg"),
col = "green", border = "white",
main = "Number of NYT Articles Published per Day",
xlab = "Randomly-Selected Days",
ylab = "Number of Articles",
axes = TRUE)
art <- c(141, 145, 255, 167, 204, 171, 354, 341, 216, 169, 189, 230, 215)
barplot(art, width = 1,
names.arg = c("Jan-2", "Feb-2", "Mar-20", "Apr-11", "May-12", "Jun-6", "Jul-26", "Aug-30", "Sept-18", "Oct-17", "Nov-10", "Dec-10", "Avg"),
col = "green", border = "white",
main = "Number of NYT Articles Published per Day",
xlab = "Randomly-Selected Days",
ylab = "Number of Articles",
axes = TRUE)
barplot(art,
names.arg = c("Jan-2", "Feb-2", "Mar-20", "Apr-11", "May-12", "Jun-6", "Jul-26", "Aug-30", "Sept-18", "Oct-17", "Nov-10", "Dec-10", "Avg"),
col = "green", border = "white",
main = "Number of NYT Articles Published per Day",
xlab = "Randomly-Selected Days",
ylab = "Number of Articles",
axes = TRUE)
barplot(art,
names.arg = c("Jan-2", "Feb-2", "Mar-20", "Apr-11", "May-12", "Jun-6", "Jul-26", "Aug-30", "Sept-18", "Oct-17", "Nov-10", "Dec-10", "Avg"),
col = "green", border = "white",
main = "Number of NYT Articles Published per Day",
xlab = "Randomly-Selected Days",
ylab = "Number of Articles",
yylim = 400,
axes = TRUE)
barplot(art,
names.arg = c("Jan-2", "Feb-2", "Mar-20", "Apr-11", "May-12", "Jun-6", "Jul-26", "Aug-30", "Sept-18", "Oct-17", "Nov-10", "Dec-10", "Avg"),
col = "green", border = "white",
main = "Number of NYT Articles Published per Day",
xlab = "Randomly-Selected Days",
ylab = "Number of Articles",
ylim = 400,
axes = TRUE)
barplot(art,
names.arg = c("Jan-2", "Feb-2", "Mar-20", "Apr-11", "May-12", "Jun-6", "Jul-26", "Aug-30", "Sept-18", "Oct-17", "Nov-10", "Dec-10", "Avg"),
col = "green", border = "white",
main = "Number of NYT Articles Published per Day",
xlab = "Randomly-Selected Days",
ylab = "Number of Articles")
hist(art,
names.arg = c("Jan-2", "Feb-2", "Mar-20", "Apr-11", "May-12", "Jun-6", "Jul-26", "Aug-30", "Sept-18", "Oct-17", "Nov-10", "Dec-10", "Avg"),
col = "green", border = "white",
main = "Number of NYT Articles Published per Day",
xlab = "Randomly-Selected Days",
ylab = "Number of Articles")
hist(art,
labels = c("Jan-2", "Feb-2", "Mar-20", "Apr-11", "May-12", "Jun-6", "Jul-26", "Aug-30", "Sept-18", "Oct-17", "Nov-10", "Dec-10", "Avg"),
col = "green", border = "white",
main = "Number of NYT Articles Published per Day",
xlab = "Randomly-Selected Days",
ylab = "Number of Articles")
warnings()
hist(art,
breaks = 11,
labels = c("Jan-2", "Feb-2", "Mar-20", "Apr-11", "May-12", "Jun-6", "Jul-26", "Aug-30", "Sept-18", "Oct-17", "Nov-10", "Dec-10", "Avg"),
col = "green", border = "white",
main = "Number of NYT Articles Published per Day",
xlab = "Randomly-Selected Days",
ylab = "Number of Articles")
hist(art,
breaks = 11,
labels = c("Jan-2", "Feb-2", "Mar-20", "Apr-11", "May-12", "Jun-6", "Jul-26", "Aug-30", "Sept-18", "Oct-17", "Nov-10", "Dec-10", "Avg"),
col = "green", border = "white",
main = "Number of NYT Articles Published per Day",
xlab = "Randomly-Selected Days",
ylab = "Number of Articles",
ylim = c(0, 400))
art <- c(141, 145, 255, 167, 204, 171, 354, 341, 216, 169, 189, 230, 215)
hist(art,
breaks = 11,
labels = c("Jan-2", "Feb-2", "Mar-20", "Apr-11", "May-12", "Jun-6", "Jul-26", "Aug-30", "Sept-18", "Oct-17", "Nov-10", "Dec-10", "Avg"),
col = "green", border = "white",
main = "Number of NYT Articles Published per Day",
xlab = "Randomly-Selected Days",
ylab = "Number of Articles",
ylim = c(0, 400))
hist(art,
breaks = 11,
freq = TRUE,
labels = c("Jan-2", "Feb-2", "Mar-20", "Apr-11", "May-12", "Jun-6", "Jul-26", "Aug-30", "Sept-18", "Oct-17", "Nov-10", "Dec-10", "Avg"),
col = "green", border = "white",
main = "Number of NYT Articles Published per Day",
xlab = "Randomly-Selected Days",
ylab = "Number of Articles",
ylim = c(0, 400))
hist(art,
breaks = 11,
freq = TRUE, probability = FALSE,
labels = c("Jan-2", "Feb-2", "Mar-20", "Apr-11", "May-12", "Jun-6", "Jul-26", "Aug-30", "Sept-18", "Oct-17", "Nov-10", "Dec-10", "Avg"),
col = "green", border = "white",
main = "Number of NYT Articles Published per Day",
xlab = "Randomly-Selected Days",
ylab = "Number of Articles",
ylim = c(0, 400))
hist(art,
breaks = 11,
freq = TRUE, probability = FALSE,
labels = c("Jan-2", "Feb-2", "Mar-20", "Apr-11", "May-12", "Jun-6", "Jul-26", "Aug-30", "Sept-18", "Oct-17", "Nov-10", "Dec-10", "Avg"),
col = "green", border = "white",
main = "Number of NYT Articles Published per Day",
xlab = "Randomly-Selected Days",
ylab = "Number of Articles")
count <- rowSums(dtm2)
count
otc <- tm_map(onlytextcorpus, content_transformer(tolower)) #need to use content_transformer with tolower because of bug in newer version of tm package
otc <- tm_map(otc, removePunctuation)
otc <- tm_map(otc, toSpace, "-")
otc <- tm_map(otc, toSpace, ":")
otc <- tm_map(otc, stripWhitespace)
dtm <- DocumentTermMatrix(otc)
dtm2 <- as.matrix(dtm)
count <- rowSums(dtm2)
count
stopwords
?stopwords
freq <- colSums(dtm2)
str(freq)
freq <- sort(freq, decreasing = TRUE)
head(freq)
head(table(freq), 30) #shows you a table of frequencies (how many words [bottom row] appear this frequently [top row])
library(wordcloud)
library(RColorBrewer)
words <- names(freq)
wordcloud(names(freq), freq, min.freq=1000, colors=brewer.pal(8, "Dark2")) #creates word cloud of words, by frequency (larger text = more), with only words that occur 1000+ times
wordcloud(names(freq), freq, min.freq=800, colors=brewer.pal(8, "Dark2")) #creates word cloud of words, by frequency (larger text = more), with only words that occur 1000+ times
wordcloud(names(freq), freq, min.freq=700, colors=brewer.pal(8, "Dark2")) #creates word cloud of words, by frequency (larger text = more), with only words that occur 1000+ times
wordcloud(names(freq), freq, min.freq=600, colors=brewer.pal(8, "Dark2")) #creates word cloud of words, by frequency (larger text = more), with only words that occur 1000+ times
head(freq, 30)
otc <- tm_map(otc, removeWords, stopwords("english"))
dtm <- DocumentTermMatrix(otc)
dtm2 <- as.matrix(dtm)
dtm # gives number of terms in documents
freq <- colSums(dtm2)
count <- rowSums(dtm2)
count
str(freq)
freq <- sort(freq, decreasing = TRUE)
head(freq, 30)
otc <- tm_map(otc, removeWords, c("url")) #insert words that you want to remove from corpus where "x" is
dtm <- DocumentTermMatrix(otc)
dtm2 <- as.matrix(dtm)
freq <- colSums(dtm2)
count <- rowSums(dtm2)
count
str(freq)
freq <- sort(freq, decreasing = TRUE)
head(freq, 30)
tail(freq, 10)
otcstem <- tm_map(otc, stemDocument)
dtmstem <- DocumentTermMatrix(otcstem)
dtmstem2 <- as.matrix(dtmstem)
freqstem <- colSums(dtmstem2)
countstem <- rowSums(dtmstem2)
counstem
countstem
library(wordcloud)
library(RColorBrewer)
wordcloud(names(freq), freq, min.freq=600, colors=brewer.pal(8, "Dark2")) #creates word cloud of words, by frequency (larger text = more), with only words that occur 1000+ times
countstem
getwd()
setwd("C:/Users/Amanda/Documents/GitHub/DataViz/Assignment1/ShinyApp")
runApp("Assignment_1_Shiny_App_Final")
library(shiny)
runApp("Assignment_1_Shiny_App_Final")
setwd("C:/Users/Amanda/Documents/GitHub/DataViz/Assignment1")
runApp("ShinyApp")
wordcloud(names(freqstem), freqstem, min.freq=600, colors=brewer.pal(8, "Dark2")) #creates word cloud of words, by frequency (larger text = more), with only words that occur 1000+ times
library(SnowballC)
countstem
freqstem <- sort(freqstem, decreasing = TRUE)
head(freqstem, 30)
library(ggplot2)
wf <- data.frame(word=names(freqstem), freq=freqstem)
p <- ggplot(subset(wf, freq>50), aes(word, freqstem))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))
p
p <- ggplot(subset(wf, freq>500), aes(word, freqstem))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))
p
p <- p + theme(axis.text.x=element_text(angle=45, hjust=329))
p
wf <- data.frame(word=names(freqstem), freq=freqstem)
p <- ggplot(subset(wf, freq>500), aes(word, freqstem))
p <- p + geom_bar(stat="identity")
p
install.packages("SnowballC")
install.packages("SnowballC")
library(SnowballC)
otcstem2 <- tm_map(otc, wordStem, language="eng") #uses SnowballC package stemming
inspect(otcstem2)
otcstem02 <- tm_map(otc, wordStem, language="eng") #uses SnowballC package stemming
otcstem0 <- tm_map(otc, wordStem, language="eng") #uses SnowballC package stemming
dtmstem0 <- DocumentTermMatrix(otcstem0) # for corpus w/o stopwords and w/ SnowballC stemming
class(otcstem0)
class(otc)
otcstem0 <- tm_map(otc, content_transformer(wordStem), language="eng") #uses SnowballC package stemming
dtmstem0 <- DocumentTermMatrix(otcstem0) # for corpus w/o stopwords and w/ SnowballC stemming
dtmstem02 <- as.matrix(dtmstem0)
freqstem0 <- colSums(dtmstem02)
countstem0 <- rowSums(dtmstem02)
countstem0
countstem
freqstem0 <- sort(freqstem0, decreasing = TRUE)
head(freqstem0, 30)
otcstem0 <- tm_map(otc, content_transformer(wordStem), language="eng") #uses SnowballC package stemming
dtmstem0 <- DocumentTermMatrix(otcstem0) # for corpus w/o stopwords and w/ SnowballC stemming
dtmstem02 <- as.matrix(dtmstem0)
freqstem0 <- colSums(dtmstem02)
countstem0 <- rowSums(dtmstem02)
countstem0
freqstem0 <- sort(freqstem0, decreasing = TRUE)
head(freqstem0, 30)
